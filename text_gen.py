# -*- coding: utf-8 -*-
"""text_gen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17yl9ZLvkDuDpGqnV45_7sb___mW-iKh1
"""

from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM, Flatten
from keras.optimizers import RMSprop
from keras.losses import SparseCategoricalCrossentropy
import numpy as np
import random 
import sys

from numba import jit, cuda
#Load text
filename = "cavegirl.txt"
raw_text = open(filename,'r',encoding='utf-8').read()
raw_text = raw_text.lower()
#print(raw_text[0:1000])

#clean text
#remove numbers
raw_text = "".join(c for c in raw_text if not c.isdigit())

#seperate words
words = []
temp = ''
for i in raw_text[:50000]:
    if i.isalpha():
        temp+=i
        continue
    if i.isspace():
        words.append(temp)
        temp = ''
        #words.append(i)
        continue
    words.append(temp)
    temp = ''
    words.append(i)  

#what are the unique wors and characters
unique_words = sorted(list(set(words)))

#encode the characters to numbers
#each char will be assigned a unique int
#create a dict for chars mapped to ints
word_to_int = dict((c,i) for i, c in enumerate(unique_words))

#do the reverse to map ints to chars
int_to_word = dict((i,c) for i, c in enumerate(unique_words))

#summarize the data
n_words = len(words)
n_vocab = len(unique_words)

print("Total words in the text: Corpus length: ", n_words)
print("Total vocab: ", n_vocab)
print(words[:30])

seq_length = 5
step = 1
sentences = []
next_words = []

for i in range(0,n_words - seq_length, step):
    sentences.append(words[i:i+seq_length])
    next_words.append(words[i+seq_length])
    
n_patterns = len(sentences)
print("Number of patterns: ", n_patterns)

# reshape the input to be [samples, time steps, features]
# WHERE time-steps = seq_length
#       features = number of chars in our vocab
# 
# vectorze all sentences: there are n_patterns sentences
# for each sentence we have n_vocab chars available for thhe seq_length
# vectorization returns a vactore of all sentences indicating the presence or 
# absence of a character


x = np.zeros((len(sentences),seq_length,n_vocab),dtype=bool)
y = np.zeros((len(sentences),n_vocab),dtype=bool)

for i, sentence in enumerate(sentences):
    for t, word in enumerate(sentence):
        x[i, t, word_to_int[word]] = 1
    y[i, word_to_int[next_words[i]]] = 1

print(x.shape)
print(y.shape)
#print(y[0:10])

#basic model with one LSTM
"""MODEL SIMPLE"""
""" simple code in here
model = Sequential()
model.add(LSTM(128, input_shape=(seq_length,n_vocab)))
model.add(Dense(n_vocab, activation="softmax"))

optimizer = keras.optimizers.RMSprop(lr=0.01)
model.compile(loss="categorical_crossentropy", optimizer='nadam')
model.summary()
"""
#deeper model with 2 LSTMs
#to stack LSTM layers, we need to change the configuration of the 
#prior LSTM layer to 3D array as nput for the subsequent layer
#we can do this by setting the return_sequence argument on the layer to true
#(defalts to false). This will return one output for each input time step and 

"""MODEL DEEP - 1"""
# model = Sequential()
# model.add(LSTM(128,input_shape=(seq_length, n_vocab),return_sequences=True))
# model.add(Dropout(0.2))
# model.add(LSTM(128))
# model.add(Dropout(0.2))
# model.add(Dense(n_vocab, activation="softmax"))
# model.compile(loss="categorical_crossentropy", optimizer=optimizer)
# model.summary() 

"""MODEL DEEP - 2"""
model = Sequential()
model.add(LSTM(128,input_shape=(seq_length, n_vocab),return_sequences=True))
model.add(Dense(128, activation="relu"))
model.add(Dropout(0.2))
model.add(Flatten(input_shape=(128,n_vocab)),)
model.add(Dense(n_vocab, activation="softmax"))

optimizer = RMSprop(learning_rate=0.01)

model.compile(optimizer=optimizer, loss="categorical_crossentropy",
             metrics=['accuracy'])
model.summary()



from keras.callbacks import ModelCheckpoint

filepath="saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.hdf5"
checkpoint = ModelCheckpoint((filepath), monitor='loss', verbose=1, save_best_only=True)
callbacks_list = [checkpoint]

#fit the model
history = model.fit(x,y, batch_size=128, epochs=8, callbacks=callbacks_list)

model.save('saved_weights_cavegirl_deep2_wordtoken0space_5sl_8epochs.h5')

from matplotlib import pyplot as plt

loss = history.history['loss']
epochs = range(1,len(loss)+1)
plt.plot(epochs, loss, 'y', label="Training loss")
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.legend()
plt.show()

def sample(preds):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds)
    exp_preds = np.exp(preds)
    #print(exp_preds,np.sum(exp_preds))
    preds = exp_preds/(np.sum(exp_preds) + 0.00000000001)
    probas = np.random.multinomial(1,preds,1)
    return np.argmax(probas)

#load the saved weights
filename = 'saved_weights_cavegirl_deep2_wordtoken0space_5sl_8epochs.h5'
model.load_weights(filename)

#pick a random sentence
start_index = random.randint(0,n_words - seq_length - 1)

generated = ''
sentence = words[start_index : start_index+seq_length]
generated.join(sentence)

print(len(sentence))
print("SEED: ",(" ".join(sentence))," --> ")

for i in range(70):
    x_pred = np.zeros((1,seq_length,n_vocab))
    for t, char in enumerate(sentence):
        x_pred[0,t,word_to_int[char]] = 1
        
    preds = model.predict(x_pred, verbose=0)[0]
    next_index = sample(preds)
    next_char = int_to_word[next_index]
    
    if(next_char == sentence[-1]):
        continue
    generated +=" "+next_char

    if sentence[-1].isalpha() and next_char.isalpha():
        sys.stdout.write(" "+next_char)
    else:
        sys.stdout.write(next_char)    
    sentence = sentence[1:] + [next_char]
    sys.stdout.flush()
    if(i>=5 and (next_char in [";",".",":","?","!","\n"])):
        break
print()

